别急，不着急学自适应感受野。

1. 线性分类器，$f(x,W)=Wx+b$ 如果想直到模型究竟学到了什么，可以直接提取 W 的每一行生成一张图，因为每个分类的 score 都是这行和 x 矩阵乘得到的（或者你说转置之后作点乘）

	线性不是直线，对于一个大小为 $[n,m]$ 的图片，我们用 $[1,n\times m]$ 的向量去表示他，也可以被理解为高维空间里面的一个点，那么这时候我们相当于用一个 $n\times m-1$ 维的超平面去切割高维空间。

2. 损失函数 $L=\frac{1}N\sum_{i=1}^n L_i$ 
	
	- 合页损失：$L_i=\sum_{j\neq y_i} \max(\Delta+s_j-s_{y_i},0)$

		这里一共 $N$ 个训练数据 $(x_i,y_i)$ ，只有在正确的分类的 score 大于错误分类 score 加 $\Delta$ 才不会给损失函数造成贡献。

		在最开始训练的时候，我们把矩阵权重设置的都很小，这时候每个类别的 score 几乎都是 0，所以每张图片的 $L_i$ 都是 $(C-1)\Delta$，其中 $C$ 是类别数量。整体的损失函数由于是所有图片的平均，所以也是接近于 $(C-1)\Delta$
	
	- 交叉熵损失：$\displaystyle L_i=\dfrac{e_{s_{y_i}}}{\sum\limits_j e^{s_j}}$ 这个函数趋向于 $1$ 的时候是最好的。但是我们对损失函数的定位是趋向于 $0^{+}$。于是取负对数。$P(Y=k|X=x_i)$ 叫似然函数，也就是从样本空间中观察到特定类别的数据的概率。$e^{s_i}$ 是非归一化概率，$\dfrac{s_i}{\sum_{j} e^{s_j}}$ 是归一化概率。
		
		最开始训练时损失函数应当是 $\log C$
		
		为了避免 score 过大带来的精度问题，需要把 $s_i$ 平移至 $\max s =0$
		
3. 注意到想要实现损失函数为 $0$ ，我们的 $W$ 可以有特别多个。这时候我们引入正则化损失 $R(W)$，表示惩罚 $W$ 中过于突出的某项特征。比如 $L1$ 正则化，$\displaystyle\sum_i\sum_j |W_{i,j}|$ 还有类似的 $L_2$ 正则化损失 $\sum\limits_{i,j} W_{i,j}^2$。都可以理解为惩罚 $W$ 中绝对值过于大的元素。举个简单例子，输入向量 $x=[1,1,1,1]$ ，两种 $x_1=[4,0,0,0],x_2=[1,1,1,1]$ 明显后者的拟合效果是要好一些的。

当然也可以引入参数把两种正则化手段结合起来：$\sum\limits_{i,j} \alpha|W_{i,j}|+\beta W_{i,j}^2$

现在损失函数应长成 $L=\frac 1N\sum L_i+\lambda R(W)$ 其中 $\alpha,\beta,\lambda\Delta$ 都是超参数。

![image](https://img2023.cnblogs.com/blog/1797571/202309/1797571-20230912084511615-206167086.png)

SVM 分类器用的是合页损失，softmax 分类器用的是 cross-entropy loss。根据损失函数的性质，我们发现交叉熵损失永不满足，但是 hinge loss 关注的是正确类别的分数比其他类别高就行了。

4. 反向传播最有意思的就是这个某个参量如果通过不同分支给最终的 $f$ 做了贡献，那么 $\dfrac {d\ f}{d\ x}$ 是这些位置的导数的加和。

5. 今天一个比较关键的理解，不知道对不对。
	
	卷积核的概念和滤波器是等价的，都叫 filter。每个滤波器的规模和上一层的通道数量应该是一致的，比如上一层是 $32\times 32$ 的 rgb 图像，那么卷积层的 filter 的大小就是 $32\times 32\times 3$。当然除了最后一维可以没有这么大（截取图像部分区域的信息），这个区域被称为感受野。感受野可以自适应，那就是更高明的算法了。
	
	卷积层会有不止一个 filter，filter的数量决定当前卷积层输出信息的通道（kernel）数量。按照 editorial 的说法，每个 filter 的感受野大小是固定且相同的，但是不同的滤波器感受野位置应该是可以相异。
	
	卷积核的参数比较神奇，每个 filter 的规模是 长 × 宽 × 通道数，但是每个通道的参数都是一样的。（但是参数的数量不是面积个，因为有偏置，要 $+1$）
	
	这就是 CNN 比较重要的想法：参数共享。我们认为某种特征在这里出现了，激活了卷积核（数学意义上理解这个激活就是说让分数变大了），那么它出现在其它地方的时候也应该激活卷积核。
	
	editorial 里面说靠前的卷积核比靠后的卷积核学习到的东西要简单一些。比如前期是边缘形状，中期有内部纹理等等。
	
	于是一个卷积层要完成的工作是：每个卷积核同上一层的输出作卷积，并且把上一层的每个深度切片的卷积结果都加起来作为当前层的一个通道的输出。反向传播要对导数作加和。
	
	一个比较有新意的东西是传统意义上的卷积，都是 $\sum f[x,y]\times g[n+x,m+y]$ 但是也可以是 $g[n+2x,m+2y]$ 原理是使用更少的参数来扩大感受野。
	
6. 池化层的反向传播：只把梯度沿着最大的传播即可。不过有人说在 GANs 里面池化层没啥用，也有人说池化层可以被卷积层替代。于是有预测说可能不再需要池化层了。

7. 这个内容比较关键了就